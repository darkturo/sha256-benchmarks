#!/bin/bash
#
# Depends: hyperfine
#

BASE_DIR=$(git rev-parse --show-toplevel)

usage() {
	echo "Usage: $0 [OPTIONS] {-c <config>|<command>}"
	echo "Options:"
	echo "  -w <warmup>  Number of warmup runs default 5"
	echo "  -r <runs>    Number of runs default 1000"
	echo "  -m <machine> Machine name default `hostname`"
	echo "  -n <name>    Name of the benchmark (derived from command if not provided)"
	echo "  -c <config>  Configuration file or a directory with a benchmark.cfg file"
        echo "               with information about the command to benchmark."
	echo "Examples:"
	echo "  $0 -c openssl/config"
	echo "  $0 -n 'openssl-custom' -i abc -- openssl dgst -sha256"
}

get_field_from_config() {
	FIELD=$1
	CONFIG=$2
	RES=$(grep -E "^$FIELD=" $CONFIG | cut -d "=" -f 2-)
	if [ -z "$RES" ]; then
		RES=""
	fi 
	if [[ "$RES" =~ '$(' ]]; then
		eval "RES=$RES"
	fi
	echo $RES
}

run_shell() {
	WARMUP=$1; shift
	RUNS=$1; shift
	NAME=$1; shift
	INPUT=$1; shift
	RESULT_PATH=$1; shift
	COMMAND=$(echo $* | sed -e 's/^"//g' -e 's/"$//g')

	echo "Benchmarking $NAME with hyperfine"
	echo "Command: $COMMAND"
	hyperfine -w $WARMUP -r $RUNS --command-name $"$NAME" --export-json $RESULT_PATH -S 'bash --norc' -- "$COMMAND < $INPUT"
	echo "Benchmark results saved to $RESULT_PATH"
}

run_python() {
	WARMUP=$1
	RUNS=$2
	NAME=$3
	INPUT=$4
	RESULT_PATH=$5
	SETUP=$6
	COMMAND=$7

	export INPUT
	PERF_OUT=pyperf-$NAME.json
	rm -f $PERF_OUT
	echo "Benchmarking $NAME with pyperf"
	python3 -m pyperf timeit --name $NAME --copy-env -o $PERF_OUT -s "$SETUP" "$COMMAND"
	pyperf stats $PERF_OUT
	mv $PERF_OUT $RESULT_PATH
	echo "Benchmark results saved to $RESULT_PATH"
}

run_bin() {
	WARMUP=$1
	RUNS=$2
	NAME=$3
	INPUT=$4
	RESULT_PATH=$5
	COMMAND_LOCATION=$6
	COMMAND=$COMMAND_LOCATION/$7

	if [ ! -f "$COMMAND" ]; then
		echo "Shell script '$COMMAND' not found. Please run cmake and build the binay benchmarks first."
	fi

#	BENCHMARK="$COMMAND --warmup $WARMUP --iterations $RUNS --input $INPUT --output $RESULT_PATH"
	BENCHMARK="$COMMAND -input $INPUT -bm_json_verbose $RESULT_PATH"
	echo "Running: $BENCHMARK"
	$BENCHMARK
	echo "Benchmark results saved to $RESULT_PATH"
}

benchmarker() {
	RESULT_PREFIX="benchmark"
	WARMUP=5
	RUNS=1000
	INPUT=abc
	CONFIG=""
	NAME=""
	MACHINE=""
	COMMAND=""

	RESULT_DIR=results
	if [ -n "$BASE_DIR" ]; then
		RESULT_DIR=$BASE_DIR/$RESULT_DIR
	fi

	while getopts "c:w:r:i:n:m:h" opt; do
		case $opt in
			c) CONFIG=$OPTARG
				;;
			w) WARMUP=$OPTARG
				;;
			r) RUNS=$OPTARG
				;;
			i) INPUT=$OPTARG
				;;
			n) NAME=$OPTARG
				;;
			m) MACHINE=$OPTARG
				;;
			h)
				usage
				exit 0
				;;
			*) 
				echo "Invalid option -$OPTARG" >&2
				usage
				exit 1
				;;
		esac
	done

	if [ -n "$CONFIG" ]; then
		if [ -d "$CONFIG" ]; then
			CONFIG=$CONFIG/benchmark.cfg
		fi
		NAME=$(get_field_from_config NAME $CONFIG)
		TYPE=$(get_field_from_config TYPE $CONFIG)
		INPUT=$(get_field_from_config INPUT $CONFIG)
		COMMAND=$(get_field_from_config COMMAND $CONFIG)
		SKIP=$(get_field_from_config SKIP $CONFIG)
	else
		echo "Error: Configuration file is required"
		usage
		exit 1
	fi
	if [ -n "$SKIP" -a "$SKIP" != "undefined" ]; then
		echo "Skipping benchmark $NAME"
		exit 0
	fi
	if [ -z "$MACHINE" ]; then
		MACHINE=$(hostname)
	fi
	if [ -z "$NAME" ]; then
		NAME=$(echo $COMMAND | cut -d " " -f 1)
	fi
	if [ -z "$INPUT" ]; then
		INPUT=abc
	fi
	INPUT=$BASE_DIR/inputs/${INPUT}.in

	DATE=$(date +'%Y%m%d')
	RESULT_PATH=$RESULT_DIR/${RESULT_PREFIX}-${NAME}-${TYPE}@${MACHINE}_${DATE}.json

	case $TYPE in 
		shell)
			run_shell $WARMUP $RUNS $NAME $INPUT $RESULT_PATH $COMMAND
			;;
		python)
			SETUP=$(get_field_from_config SETUP $CONFIG)
			run_python $WARMUP $RUNS $NAME $INPUT $RESULT_PATH $"$SETUP" $"$COMMAND"
			;;
		bin)
			COMMAND_LOCATION=$(dirname $CONFIG)
			run_bin $WARMUP $RUNS $NAME $SKIP $INPUT $RESULT_PATH $COMMAND_LOCATION $COMMAND
			;;
		*)
			echo "Error: Unsupported benchmark type $TYPE"
			exit 1
			;;
	esac
}

if [ "$0" = "$BASH_SOURCE" ]; then
	benchmarker "$@"
fi
